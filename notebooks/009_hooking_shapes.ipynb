{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking constant-sized tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling models in torch with `torch.export` is a bit of a hassle because conditionals are not well supported. As such, we must try to find constant tensors, be it by shape or value and try to hint the compiler about it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dgcnz/development/amsterdam/edge\n"
     ]
    }
   ],
   "source": [
    "from detectron2.data.detection_utils import read_image\n",
    "from demo.predictors import VisualizationDemo\n",
    "from detectron2.config import LazyConfig, instantiate\n",
    "from pathlib import Path\n",
    "from detectron2.checkpoint import DetectionCheckpointer\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from copy import copy, deepcopy\n",
    "import detectron2.data.transforms as T\n",
    "from detectron2.utils.visualizer import ColorMode, Visualizer\n",
    "from detectron2.data import MetadataCatalog\n",
    "from torch.export import export\n",
    "import typing\n",
    "from torch.export import Dim, export\n",
    "from torchvision.transforms import functional as TTF\n",
    "from typing import Callable, Tuple\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "print(Path.cwd())\n",
    "assert (Path.cwd() / \".project-root\").exists(), \"Please run this script from the root of the project\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgcnz/development/amsterdam/edge/detrex/detrex/layers/dcn_v3.py:24: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(\n",
      "/Users/dgcnz/development/amsterdam/edge/detrex/detrex/layers/dcn_v3.py:53: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n"
     ]
    }
   ],
   "source": [
    "config_file = \"projects/dino_dinov2/configs/COCO/dino_dinov2_b_12ep.py\"\n",
    "image_path = \"artifacts/idea_raw.jpg\"\n",
    "CONFIDENCE_THRESHOLD = 0.5\n",
    "opts = [\"model.device=cpu\", \"train.device=cpu\", \"train.init_checkpoint=artifacts/model_final.pth\"]\n",
    "cfg = LazyConfig.load(config_file)\n",
    "cfg = LazyConfig.apply_overrides(cfg, opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[08/19 18:27:02 timm backbone]: \u001b[0mbackbone out_indices: (11,)\n",
      "\u001b[32m[08/19 18:27:02 timm backbone]: \u001b[0mbackbone out_channels: [768]\n",
      "\u001b[32m[08/19 18:27:02 timm backbone]: \u001b[0mbackbone out_strides: [16]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dgcnz/development/amsterdam/edge/.venv/lib/python3.10/site-packages/fvcore/common/checkpoint.py:252: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "model = instantiate(OmegaConf.to_object(cfg.model)).eval();\n",
    "checkpointer = DetectionCheckpointer(model);\n",
    "checkpointer.load(cfg.train.init_checkpoint);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = read_image(image_path, format=\"BGR\")\n",
    "demo = VisualizationDemo(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T1: `spatial_shapes` over multiple image sizes\n",
    "\n",
    "`spatial_shapes` has been one of the main culprits in compiler errors, and in testing it has been constant so far. However, testing has been with a single sample image, so it might be the case that multiple image sizes yield multiple spatial shapes.\n",
    "\n",
    "So that's what we will do, we will reshape our sample image in multiple sizes and hook a logger to a layer taking `spatial_shapes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_shape = img.shape # (1920, 1281, 3)\n",
    "new_shapes = [(200, 2000),(800, 800), (800, 1200), (1200, 800), (1200, 1200), (2000, 2000)]\n",
    "img_cwh = torch.from_numpy(img.copy()).permute(2, 0, 1)\n",
    "images = [TTF.resize(img_cwh, new_shape).permute(1, 2, 0).numpy() for new_shape in new_shapes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_shapes_history = []\n",
    "\n",
    "def track_spatial_shapes(layer: nn.Module, inputs: Tuple[torch.Tensor], kwargs: dict):\n",
    "    global spatial_shapes_history\n",
    "    spatial_shapes_history.append(kwargs[\"spatial_shapes\"])\n",
    "\n",
    "\n",
    "handle = demo.predictor.model.transformer.encoder.register_forward_pre_hook(track_spatial_shapes, with_kwargs=True)\n",
    "\n",
    "for _img in images:\n",
    "    demo.run_on_image(_img)\n",
    "\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((200, 2000), [18, 168, 9, 84, 4, 42, 2, 21])\n",
      "((800, 800), [100, 100, 50, 50, 25, 25, 13, 13])\n",
      "((800, 1200), [100, 150, 50, 75, 25, 37, 13, 19])\n",
      "((1200, 800), [150, 100, 75, 50, 37, 25, 19, 13])\n",
      "((1200, 1200), [100, 100, 50, 50, 25, 25, 13, 13])\n",
      "((2000, 2000), [100, 100, 50, 50, 25, 25, 13, 13])\n"
     ]
    }
   ],
   "source": [
    "print(*[(new_shapes[ix], x.flatten().tolist()) for ix, x in enumerate(spatial_shapes_history)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's unfortunate, it seems that `spatial_shapes` is not constant and there's no trivial pattern to see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### T2: `spatial_shapes` over a single image size\n",
    "\n",
    "However, if we think about the production usage, we will use at most 2 different cameras (which presumably have the same resolution), so we might as well just compile a model specifically to that resolution. Just to sanity check, let's run different images of the same size and see if `spatial_shapes` change."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_uint8_image(shape: Tuple[int, int, int]) -> np.ndarray:\n",
    "    return np.random.randint(0, 255, shape, dtype=np.uint8)\n",
    "\n",
    "# random_uint8_image(img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_IMAGES = 4\n",
    "images = [random_uint8_image(img.shape) for _ in range(N_IMAGES)]\n",
    "images.append(np.zeros(img.shape, dtype=np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatial_shapes_history = []\n",
    "\n",
    "def track_spatial_shapes(layer: nn.Module, inputs: Tuple[torch.Tensor], kwargs: dict):\n",
    "    global spatial_shapes_history\n",
    "    spatial_shapes_history.append(kwargs[\"spatial_shapes\"])\n",
    "\n",
    "\n",
    "handle = demo.predictor.model.transformer.encoder.register_forward_pre_hook(track_spatial_shapes, with_kwargs=True)\n",
    "\n",
    "for _img in images:\n",
    "    demo.run_on_image(_img)\n",
    "\n",
    "handle.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150, 100, 75, 50, 37, 25, 19, 13]\n",
      "[150, 100, 75, 50, 37, 25, 19, 13]\n",
      "[150, 100, 75, 50, 37, 25, 19, 13]\n",
      "[150, 100, 75, 50, 37, 25, 19, 13]\n",
      "[150, 100, 75, 50, 37, 25, 19, 13]\n"
     ]
    }
   ],
   "source": [
    "print(*[x.flatten().tolist() for ix, x in enumerate(spatial_shapes_history)], sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems fine, we'll do that."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
