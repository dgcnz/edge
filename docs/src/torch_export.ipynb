{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 2 Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general idea of `torch.export` is that it translates an Eager Mode PyTorch model into a graph-based intermediate representation called Export IR. This allows compiler backends to take this IR and further transform and optimize it for a target device. A general overview of the process is shown in the figure [below](torchexport).\n",
    "\n",
    ":::{figure-md} torchexport\n",
    "<img src=\"compilation.png\" alt=\"torch.export\">\n",
    "\n",
    "PyTorch 2 Export\n",
    ":::\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, \n",
    "\n",
    " for compilers in a few ways:\n",
    "1. Operators have to be functional, so that compilers don't have to deal with side effects.\n",
    "2. Operators have to be general enough for backends to notice patterns and optimize them.\n",
    "3. The number of operators has to be small enough for the backend to implement all of them.\n",
    "\n",
    "With this IR, compilers can take the graph representing the model and perform multiple optimizations on it, like fusing operators (merging multiple operators into one that has a specialized kernel, useful to avoid multiple CUDA kernel calls and just do it ones), partitioning the graph (splitting the graph into multiple subgraphs that can be executed in parallel), etc. We'll talk about these \"passes\", because they will inform the user on how to debug the model if something goes wrong.\n",
    "\n",
    "For now, let's get some practical intuition with an example.\n",
    "\n",
    "Let's use the following simple network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(3, 6, 5)\n",
    "        self.fc1 = nn.Linear(6 * 14 * 14, 10)\n",
    "        self.fc2 = nn.Linear(6 * 14 * 14, 10)\n",
    "        self.register_buffer(\"mask\", torch.randn(6, 14, 14) > 0.5)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, (2, 2))\n",
    "        x = x * self.mask\n",
    "        x = torch.flatten(x, 1)\n",
    "        y = self.fc1(x)\n",
    "        z = self.fc2(x)\n",
    "        y = F.relu(y) \n",
    "        z = F.relu(z)\n",
    "        return z, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 32, 32) \n",
    "ep: torch.export.ExportedProgram = torch.export.export(SimpleNet(), (x,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.save(ep, \"simple_net.pt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
