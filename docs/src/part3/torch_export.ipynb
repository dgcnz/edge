{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch 2 Export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```{contents}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.export 101\n",
    "\n",
    "The main idea of `torch.export` is that it translates an Eager Mode PyTorch model into a graph-based intermediate representation called *Export IR*. This allows compiler backends to take this IR and further transform and optimize it for a target device. A general overview of the process is shown in the figure [below](torchexport).\n",
    "\n",
    ":::{figure-md} torchexport\n",
    "<img src=\"compilation.png\" alt=\"torch.export\" width=70%>\n",
    "\n",
    "PyTorch 2 Export\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This IR needs to fulfill a couple of properties for it to be useful to compilers. For example:\n",
    "1. Operators have to be general enough for backends to notice patterns and optimize them: Many runtimes have specialized kernels  for common operators like convolutions or even more complex ones like a `conv2 + batchnorm` (operator fusion). If the IR reduces all operators to sums, products and views, noticing these patterns becomes too hard.\n",
    "2. The number of operators has to be small enough for the backend to implement all of them. \n",
    "3. Operators have to be functional, that is, without side effects. For example: If two functions read and modify the same parameters, the order of execution matters and the compiler has to be careful when parallelizing them.\n",
    "\n",
    "Notice that properties 1 and 2 are in conflict with each other. The more operators we have, the more expressive the IR is, but the harder it is to implement all of them. This is a trade-off that the PyTorch team has to balance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [ ] Introduce ATEN (dialects), fx.Graph and link to Export IR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For now, let's get some practical intuition with an example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hands on with torch.export"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use a simple network to see how `torch.export` works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pprint\n",
    "from part3_artifacts.simple_net import SimpleNet\n",
    "import torch.fx.graph_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mInit signature:\u001b[0m \u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSource:\u001b[0m        \n",
      "\u001b[0;32mclass\u001b[0m \u001b[0mSimpleNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m\"\"\"\u001b[0m\n",
      "\u001b[0;34m    Just a simple network\u001b[0m\n",
      "\u001b[0;34m    \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5184\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m        \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFile:\u001b[0m           ~/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py\n",
      "\u001b[0;31mType:\u001b[0m           type\n",
      "\u001b[0;31mSubclasses:\u001b[0m     "
     ]
    }
   ],
   "source": [
    "SimpleNet??"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To export a model we must first define a sample input. This is used to trace the model and generate the Export IR. The way this works efficiently is by using `torch._subclasses.fake_tensor.FakeTensor`. FakeTensors are a special type of tensor that only store metadata such as `dtype`, `shape` and `device` and overload all operators to simulate the computation without actually looking at the values. For example, doing matrix multiplications of FakeTensors of shapes `(N, M)` and `(M, K)` will return a FakeTensor of shape `(N, K)` in constant time instead of the normal quadratic complexity.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1, 3, 32, 32) \n",
    "ep: torch.export.ExportedProgram = torch.export.export(SimpleNet().eval(), (x,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it, we have exported our model. The new object is a `torch.export.ExportedProgram` which contains the model and parameters in the Export IR. Let's inspect it one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first and most important attribute is the `graph_module` which stores the computational graph of the model. We can print it using the `print_readable` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, p_conv1_weight: \"\u001b[31mf32\u001b[0m\u001b[34m[6, 3, 5, 5]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", p_conv1_bias: \"\u001b[31mf32\u001b[0m\u001b[34m[6]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", p_conv2_weight: \"\u001b[31mf32\u001b[0m\u001b[34m[9, 6, 5, 5]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", p_conv2_bias: \"\u001b[31mf32\u001b[0m\u001b[34m[9]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", p_fc_weight: \"\u001b[31mf32\u001b[0m\u001b[34m[10, 5184]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", p_fc_bias: \"\u001b[31mf32\u001b[0m\u001b[34m[10]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\", x: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 3, 32, 32]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\"):\n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:16 in forward, code: x = self.conv1(x)\u001b[0m\n",
      "        conv2d: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 6, 28, 28]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.conv2d.default(x, p_conv1_weight, p_conv1_bias);  \u001b[2mx = p_conv1_weight = p_conv1_bias = None\u001b[0m\n",
      "        \n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:17 in forward, code: x = F.relu(x)\u001b[0m\n",
      "        relu: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 6, 28, 28]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.relu.default(conv2d);  \u001b[2mconv2d = None\u001b[0m\n",
      "        \n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:18 in forward, code: x = self.conv2(x)\u001b[0m\n",
      "        conv2d_1: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 9, 24, 24]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.conv2d.default(relu, p_conv2_weight, p_conv2_bias);  \u001b[2mrelu = p_conv2_weight = p_conv2_bias = None\u001b[0m\n",
      "        \n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:19 in forward, code: x = F.relu(x)\u001b[0m\n",
      "        relu_1: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 9, 24, 24]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.relu.default(conv2d_1);  \u001b[2mconv2d_1 = None\u001b[0m\n",
      "        \n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:20 in forward, code: x = torch.flatten(x, 1)\u001b[0m\n",
      "        view: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 5184]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.view.default(relu_1, \u001b[34m[1, 5184]\u001b[0m);  \u001b[2mrelu_1 = None\u001b[0m\n",
      "        \n",
      "         \u001b[2m# File: /home/dgcnz/development/amsterdam/edge/docs/src/part3/part3_artifacts/simple_net.py:21 in forward, code: x = self.fc(x)\u001b[0m\n",
      "        linear: \"\u001b[31mf32\u001b[0m\u001b[34m[1, 10]\u001b[0m\u001b[2m\u001b[34m\u001b[0m\u001b[2m\u001b[32mcpu\u001b[0m\" = torch.ops.aten.linear.default(view, p_fc_weight, p_fc_bias);  \u001b[2mview = p_fc_weight = p_fc_bias = None\u001b[0m\n",
      "        return (linear,)\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "graph_module: torch.fx.GraphModule = ep.graph_module\n",
    "print(graph_module.print_readable(print_output=False, colored=True, include_device=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see all nodes (`conv2d`, `relu`, `conv2d_1`, etc.), their shapes, dtypes, devices and the aten operators that are being used (`torch.ops.aten.conv2d.default`) with their accompanying file, line and code. We can also see that the graph inputs expects not only the model inputs but also its parameters (buffers and constants too)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `torch.fx.GraphModule` is just a wrapper around the `fx.Graph`, and you can access it through `graph_module.graph`. This is useful because `fx.Graph` has a lot of methods to manipulate the graph, like `graph_module.graph.nodes` to access all nodes, `graph_module.graph.nodes[0].args` to access the arguments of the first node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph():\n",
      "    %p_conv1_weight : [num_users=1] = placeholder[target=p_conv1_weight]\n",
      "    %p_conv1_bias : [num_users=1] = placeholder[target=p_conv1_bias]\n",
      "    %p_conv2_weight : [num_users=1] = placeholder[target=p_conv2_weight]\n",
      "    %p_conv2_bias : [num_users=1] = placeholder[target=p_conv2_bias]\n",
      "    %p_fc_weight : [num_users=1] = placeholder[target=p_fc_weight]\n",
      "    %p_fc_bias : [num_users=1] = placeholder[target=p_fc_bias]\n",
      "    %x : [num_users=1] = placeholder[target=x]\n",
      "    %conv2d : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%x, %p_conv1_weight, %p_conv1_bias), kwargs = {})\n",
      "    %relu : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%conv2d,), kwargs = {})\n",
      "    %conv2d_1 : [num_users=1] = call_function[target=torch.ops.aten.conv2d.default](args = (%relu, %p_conv2_weight, %p_conv2_bias), kwargs = {})\n",
      "    %relu_1 : [num_users=1] = call_function[target=torch.ops.aten.relu.default](args = (%conv2d_1,), kwargs = {})\n",
      "    %view : [num_users=1] = call_function[target=torch.ops.aten.view.default](args = (%relu_1, [1, 5184]), kwargs = {})\n",
      "    %linear : [num_users=1] = call_function[target=torch.ops.aten.linear.default](args = (%view, %p_fc_weight, %p_fc_bias), kwargs = {})\n",
      "    return (linear,)\n"
     ]
    }
   ],
   "source": [
    "print(graph_module.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[p_conv1_weight,\n",
      " p_conv1_bias,\n",
      " p_conv2_weight,\n",
      " p_conv2_bias,\n",
      " p_fc_weight,\n",
      " p_fc_bias,\n",
      " x,\n",
      " conv2d,\n",
      " relu,\n",
      " conv2d_1,\n",
      " relu_1,\n",
      " view,\n",
      " linear,\n",
      " output]\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(list(graph_module.graph.nodes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "def forward(self, p_conv1_weight, p_conv1_bias, p_conv2_weight, p_conv2_bias, p_fc_weight, p_fc_bias, x):\n",
      "    conv2d = torch.ops.aten.conv2d.default(x, p_conv1_weight, p_conv1_bias);  x = p_conv1_weight = p_conv1_bias = None\n",
      "    relu = torch.ops.aten.relu.default(conv2d);  conv2d = None\n",
      "    conv2d_1 = torch.ops.aten.conv2d.default(relu, p_conv2_weight, p_conv2_bias);  relu = p_conv2_weight = p_conv2_bias = None\n",
      "    relu_1 = torch.ops.aten.relu.default(conv2d_1);  conv2d_1 = None\n",
      "    view = torch.ops.aten.view.default(relu_1, [1, 5184]);  relu_1 = None\n",
      "    linear = torch.ops.aten.linear.default(view, p_fc_weight, p_fc_bias);  view = p_fc_weight = p_fc_bias = None\n",
      "    return (linear,)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(graph_module.graph.python_code(graph_module.graph._root).src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExportGraphSignature(input_specs=[InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_conv1_weight'),\n",
      "                                            target='conv1.weight',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_conv1_bias'),\n",
      "                                            target='conv1.bias',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_conv2_weight'),\n",
      "                                            target='conv2.weight',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_conv2_bias'),\n",
      "                                            target='conv2.bias',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_fc_weight'),\n",
      "                                            target='fc.weight',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.PARAMETER: 2>,\n",
      "                                            arg=TensorArgument(name='p_fc_bias'),\n",
      "                                            target='fc.bias',\n",
      "                                            persistent=None),\n",
      "                                  InputSpec(kind=<InputKind.USER_INPUT: 1>,\n",
      "                                            arg=TensorArgument(name='x'),\n",
      "                                            target=None,\n",
      "                                            persistent=None)],\n",
      "                     output_specs=[OutputSpec(kind=<OutputKind.USER_OUTPUT: 1>,\n",
      "                                              arg=TensorArgument(name='linear'),\n",
      "                                              target=None)])\n"
     ]
    }
   ],
   "source": [
    "pprint.pp(ep._graph_signature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'fc.weight', 'fc.bias'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep._state_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.export.save(ep, \"simple_net.pt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [\n",
    "    torch.rand(1, 3, 150, 100),\n",
    "    torch.rand(1, 3, 75, 50),\n",
    "    torch.rand(1, 3, 37, 25),\n",
    "    torch.rand(1, 3, 19, 13),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ep.constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cu124",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
